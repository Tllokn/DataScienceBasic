{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'cnn_p2' from '/Users/sunluzhe/Desktop/LearningInUchicago/2022SpringQuarter/Machine Learning/hw/hw5/codequestion/cnn_p2.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "import time\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cnn_p2 as cnn  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "importlib.reload(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will implement the following CNN network from scratch\n",
    "- The following **optional** code is from the [CNN tutorial at PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).  It illustrates how the network is specified in PyTorch, and counts the number of parameters.  You will have to install PyTorch for it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable params in each layer:\n",
      "   conv1.weight 450\n",
      "   conv1.bias 6\n",
      "   conv2.weight 2,400\n",
      "   conv2.bias 16\n",
      "   fc1.weight 48,000\n",
      "   fc1.bias 120\n",
      "   fc2.weight 10,080\n",
      "   fc2.bias 84\n",
      "   fc3.weight 840\n",
      "   fc3.bias 10\n",
      "Total trainable params: 62,006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62006"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# This is OPTIONAL code.  It will not run unless\n",
    "# PyTorch is installed.\n",
    "#\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    print(f\"The number of trainable params in each layer:\")\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        print('  ', name, f'{params:,d}')\n",
    "        total_params+=params\n",
    "    print(f\"Total trainable params: {total_params:,d}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Below are test cases to help you debug the Conv, MaxPool and Flatten\n",
    "- For HW5, you will also rely on other Operation Classes (VDot, Softmax, Log, etc) that you have implemented in HW4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class FailTestError(Exception):\n",
    "    pass\n",
    "\n",
    "input_tensor = cnn.InputValue(np.arange(4*4*2).reshape((4,4,2)))\n",
    "conv1 = cnn.InputValue(np.arange(3*3*2*4).reshape((3,3,2,4)))\n",
    "bias1 = cnn.InputValue(np.arange(4))\n",
    "v1 = cnn.InputValue(np.arange(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test on Conv with default settings\n"
     ]
    }
   ],
   "source": [
    "# Test Conv\n",
    "x = cnn.Add(input_tensor, input_tensor)\n",
    "y = cnn.Conv(x, conv1, 1, 0)\n",
    "z = cnn.Add(y, bias1)\n",
    "for component in [x,y,z]:\n",
    "    component.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "for component in [x, y, conv1, bias1]:\n",
    "    component.grad = 0\n",
    "for component in [z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "yvalue = np.array([[[17880., 18258., 18636., 19014.],\n",
    "                    [20328., 20778., 21228., 21678.]],\n",
    "\n",
    "                   [[27672., 28338., 29004., 29670.],\n",
    "                    [30120., 30858., 31596., 32334.]]])\n",
    "y_kernel_grad = np.array([[[[ 40.,  40.,  40.,  40.],\n",
    "                             [ 48.,  48.,  48.,  48.]],\n",
    "\n",
    "                            [[ 56.,  56.,  56.,  56.],\n",
    "                             [ 64.,  64.,  64.,  64.]],\n",
    "\n",
    "                            [[ 72.,  72.,  72.,  72.],\n",
    "                             [ 80.,  80.,  80.,  80.]]],\n",
    "\n",
    "\n",
    "                           [[[104., 104., 104., 104.],\n",
    "                             [112., 112., 112., 112.]],\n",
    "\n",
    "                            [[120., 120., 120., 120.],\n",
    "                             [128., 128., 128., 128.]],\n",
    "\n",
    "                            [[136., 136., 136., 136.],\n",
    "                             [144., 144., 144., 144.]]],\n",
    "\n",
    "\n",
    "                           [[[168., 168., 168., 168.],\n",
    "                             [176., 176., 176., 176.]],\n",
    "\n",
    "                            [[184., 184., 184., 184.],\n",
    "                             [192., 192., 192., 192.]],\n",
    "\n",
    "                            [[200., 200., 200., 200.],\n",
    "                             [208., 208., 208., 208.]]]])\n",
    "y_inputtensor_grad = np.array([[[  6.,  22.],\n",
    "                                [ 44.,  76.],\n",
    "                                [108., 140.],\n",
    "                                [ 70.,  86.]],\n",
    "\n",
    "                               [[108., 140.],\n",
    "                                [280., 344.],\n",
    "                                [408., 472.],\n",
    "                                [236., 268.]],\n",
    "\n",
    "                               [[300., 332.],\n",
    "                                [664., 728.],\n",
    "                                [792., 856.],\n",
    "                                [428., 460.]],\n",
    "\n",
    "                               [[198., 214.],\n",
    "                                [428., 460.],\n",
    "                                [492., 524.],\n",
    "                                [262., 278.]]])\n",
    "\n",
    "if not np.array_equal(y.value, yvalue):\n",
    "    raise FailTestError(\"The output of Conv is incorrect\")\n",
    "if not np.array_equal(y.kernel.grad, y_kernel_grad):\n",
    "    raise FailTestError(\"The gradient of kernel in Conv is incorrect\")\n",
    "if not np.array_equal(y.input_tensor.grad, y_inputtensor_grad):\n",
    "    raise FailTestError(\"The gradient of input_tensor in Conv is incorrect\")\n",
    "print(\"Passed test on Conv with default settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Credit: Passed Test on Conv\n"
     ]
    }
   ],
   "source": [
    "# Additional Test Conv\n",
    "m1 = cnn.InputValue(np.arange(2*2*4).reshape((2,2,4)))\n",
    "x = cnn.Add(input_tensor, input_tensor)\n",
    "y = cnn.Conv(x, conv1, 1, 0)\n",
    "z = cnn.Add(y, bias1)\n",
    "u = cnn.Mul(z, m1)\n",
    "for component in [x,y,z,u]:\n",
    "    component.forward()\n",
    "u.grad = np.ones_like(u.value)\n",
    "for component in [x, y, z, conv1, bias1]:\n",
    "    component.grad = 0\n",
    "for component in [u,z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "yvalue = np.array([[[17880., 18258., 18636., 19014.],\n",
    "                    [20328., 20778., 21228., 21678.]],\n",
    "\n",
    "                   [[27672., 28338., 29004., 29670.],\n",
    "                    [30120., 30858., 31596., 32334.]]])\n",
    "y_kernel_grad = np.array([[[[ 384.,  424.,  464.,  504.],\n",
    "                             [ 432.,  480.,  528.,  576.]],\n",
    "\n",
    "                            [[ 480.,  536.,  592.,  648.],\n",
    "                             [ 528.,  592.,  656.,  720.]],\n",
    "\n",
    "                            [[ 576.,  648.,  720.,  792.],\n",
    "                             [ 624.,  704.,  784.,  864.]]],\n",
    "\n",
    "\n",
    "                           [[[ 768.,  872.,  976., 1080.],\n",
    "                             [ 816.,  928., 1040., 1152.]],\n",
    "\n",
    "                            [[ 864.,  984., 1104., 1224.],\n",
    "                             [ 912., 1040., 1168., 1296.]],\n",
    "\n",
    "                            [[ 960., 1096., 1232., 1368.],\n",
    "                             [1008., 1152., 1296., 1440.]]],\n",
    "\n",
    "\n",
    "                           [[[1152., 1320., 1488., 1656.],\n",
    "                             [1200., 1376., 1552., 1728.]],\n",
    "\n",
    "                            [[1248., 1432., 1616., 1800.],\n",
    "                             [1296., 1488., 1680., 1872.]],\n",
    "\n",
    "                            [[1344., 1544., 1744., 1944.],\n",
    "                             [1392., 1600., 1808., 2016.]]]])\n",
    "y_inputtensor_grad = np.array([[[  14.,   38.],\n",
    "                                [ 100.,  212.],\n",
    "                                [ 324.,  436.],\n",
    "                                [ 390.,  478.]],\n",
    "\n",
    "                               [[ 220.,  396.],\n",
    "                                [1224., 1704.],\n",
    "                                [2184., 2664.],\n",
    "                                [1868., 2172.]],\n",
    "\n",
    "                               [[1276., 1452.],\n",
    "                                [4104., 4584.],\n",
    "                                [5064., 5544.],\n",
    "                                [3692., 3996.]],\n",
    "\n",
    "                               [[1886., 2038.],\n",
    "                                [4868., 5236.],\n",
    "                                [5604., 5972.],\n",
    "                                [3542., 3758.]]])\n",
    "\n",
    "if not np.array_equal(y.value, yvalue):\n",
    "    raise FailTestError(\"The output of Conv is incorrect\")\n",
    "if not np.array_equal(y.kernel.grad, y_kernel_grad):\n",
    "    raise FailTestError(\"The gradient of kernel in Conv is incorrect\")\n",
    "if not np.array_equal(y.input_tensor.grad, y_inputtensor_grad):\n",
    "    raise FailTestError(\"The gradient of input_tensor in Conv is incorrect\")\n",
    "print(\"Extra Credit: Passed Test on Conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on Conv with stride 1\n"
     ]
    }
   ],
   "source": [
    "# For Full Credit: Conv with stride 1\n",
    "x = cnn.Add(input_tensor, input_tensor)\n",
    "y = cnn.Conv(x, conv1, 1, 1)\n",
    "z = cnn.Add(y, bias1)\n",
    "for component in [x,y,z]:\n",
    "    component.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "for component in [x, y, conv1, bias1]:\n",
    "    component.grad = 0\n",
    "for component in [z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "yvalue = np.array([[[ 5248.,  5336.,  5424.,  5512.],\n",
    "                    [ 8608.,  8764.,  8920.,  9076.],\n",
    "                    [10816., 11020., 11224., 11428.],\n",
    "                    [ 7232.,  7384.,  7536.,  7688.]],\n",
    "\n",
    "                   [[11856., 12084., 12312., 12540.],\n",
    "                    [17880., 18258., 18636., 19014.],\n",
    "                    [20328., 20778., 21228., 21678.],\n",
    "                    [12912., 13236., 13560., 13884.]],\n",
    "\n",
    "                   [[19152., 19572., 19992., 20412.],\n",
    "                    [27672., 28338., 29004., 29670.],\n",
    "                    [30120., 30858., 31596., 32334.],\n",
    "                    [18672., 19188., 19704., 20220.]],\n",
    "\n",
    "                   [[ 9792., 10136., 10480., 10824.],\n",
    "                    [13312., 13852., 14392., 14932.],\n",
    "                    [14368., 14956., 15544., 16132.],\n",
    "                    [ 8192.,  8600.,  9008.,  9416.]]])\n",
    "y_kernel_grad = np.array([[[[180., 180., 180., 180.],\n",
    "                         [198., 198., 198., 198.]],\n",
    "\n",
    "                        [[264., 264., 264., 264.],\n",
    "                         [288., 288., 288., 288.]],\n",
    "\n",
    "                        [[216., 216., 216., 216.],\n",
    "                         [234., 234., 234., 234.]]],\n",
    "\n",
    "\n",
    "                       [[[336., 336., 336., 336.],\n",
    "                         [360., 360., 360., 360.]],\n",
    "\n",
    "                        [[480., 480., 480., 480.],\n",
    "                         [512., 512., 512., 512.]],\n",
    "\n",
    "                        [[384., 384., 384., 384.],\n",
    "                         [408., 408., 408., 408.]]],\n",
    "\n",
    "\n",
    "                       [[[324., 324., 324., 324.],\n",
    "                         [342., 342., 342., 342.]],\n",
    "\n",
    "                        [[456., 456., 456., 456.],\n",
    "                         [480., 480., 480., 480.]],\n",
    "\n",
    "                        [[360., 360., 360., 360.],\n",
    "                         [378., 378., 378., 378.]]]])\n",
    "y_inputtensor_grad = np.array([[[ 280.,  344.],\n",
    "                                [ 516.,  612.],\n",
    "                                [ 516.,  612.],\n",
    "                                [ 408.,  472.]],\n",
    "\n",
    "                               [[ 708.,  804.],\n",
    "                                [1206., 1350.],\n",
    "                                [1206., 1350.],\n",
    "                                [ 900.,  996.]],\n",
    "\n",
    "                               [[ 708.,  804.],\n",
    "                                [1206., 1350.],\n",
    "                                [1206., 1350.],\n",
    "                                [ 900.,  996.]],\n",
    "\n",
    "                               [[ 664.,  728.],\n",
    "                                [1092., 1188.],\n",
    "                                [1092., 1188.],\n",
    "                                [ 792.,  856.]]])\n",
    "\n",
    "if not np.array_equal(y.value, yvalue):\n",
    "    raise FailTestError(\"The output of Conv is incorrect\")\n",
    "if not np.array_equal(y.kernel.grad, y_kernel_grad):\n",
    "    raise FailTestError(\"The gradient of kernel in Conv is incorrect\")\n",
    "if not np.array_equal(y.input_tensor.grad, y_inputtensor_grad):\n",
    "    raise FailTestError(\"The gradient of input_tensor in Conv is incorrect\")\n",
    "print(\"Passed Test on Conv with stride 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on Conv with stride 2 and padding 1\n"
     ]
    }
   ],
   "source": [
    "# For Full Credit: Conv with stride 2 and padding 1\n",
    "x = cnn.Add(input_tensor, input_tensor)\n",
    "y = cnn.Conv(x, conv1, 2, 1)\n",
    "z = cnn.Add(y, bias1)\n",
    "for component in [x,y,z]:\n",
    "    component.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "for component in [x, y, conv1, bias1]:\n",
    "    component.grad = 0\n",
    "for component in [z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "yvalue = np.array([[[ 5248.,  5336.,  5424.,  5512.],\n",
    "                    [10816., 11020., 11224., 11428.]],\n",
    "\n",
    "                   [[19152., 19572., 19992., 20412.],\n",
    "                    [30120., 30858., 31596., 32334.]]])\n",
    "y_kernel_grad = np.array([[[[ 20.,  20.,  20.,  20.],\n",
    "                             [ 22.,  22.,  22.,  22.]],\n",
    "\n",
    "                            [[ 40.,  40.,  40.,  40.],\n",
    "                             [ 44.,  44.,  44.,  44.]],\n",
    "\n",
    "                            [[ 48.,  48.,  48.,  48.],\n",
    "                             [ 52.,  52.,  52.,  52.]]],\n",
    "\n",
    "\n",
    "                           [[[ 40.,  40.,  40.,  40.],\n",
    "                             [ 44.,  44.,  44.,  44.]],\n",
    "\n",
    "                            [[ 80.,  80.,  80.,  80.],\n",
    "                             [ 88.,  88.,  88.,  88.]],\n",
    "\n",
    "                            [[ 96.,  96.,  96.,  96.],\n",
    "                             [104., 104., 104., 104.]]],\n",
    "\n",
    "\n",
    "                           [[[ 72.,  72.,  72.,  72.],\n",
    "                             [ 76.,  76.,  76.,  76.]],\n",
    "\n",
    "                            [[144., 144., 144., 144.],\n",
    "                             [152., 152., 152., 152.]],\n",
    "\n",
    "                            [[160., 160., 160., 160.],\n",
    "                             [168., 168., 168., 168.]]]])\n",
    "y_inputtensor_grad = np.array([[[134., 150.],\n",
    "                                [268., 300.],\n",
    "                                [134., 150.],\n",
    "                                [166., 182.]],\n",
    "\n",
    "                               [[268., 300.],\n",
    "                                [536., 600.],\n",
    "                                [268., 300.],\n",
    "                                [332., 364.]],\n",
    "\n",
    "                               [[134., 150.],\n",
    "                                [268., 300.],\n",
    "                                [134., 150.],\n",
    "                                [166., 182.]],\n",
    "\n",
    "                               [[230., 246.],\n",
    "                                [460., 492.],\n",
    "                                [230., 246.],\n",
    "                                [262., 278.]]])\n",
    "\n",
    "if not np.array_equal(y.value, yvalue):\n",
    "    raise FailTestError(\"The output of Conv is incorrect\")\n",
    "if not np.array_equal(y.kernel.grad, y_kernel_grad):\n",
    "    raise FailTestError(\"The gradient of kernel in Conv is incorrect\")\n",
    "if not np.array_equal(y.input_tensor.grad, y_inputtensor_grad):\n",
    "    raise FailTestError(\"The gradient of input_tensor in Conv is incorrect\")\n",
    "print(\"Passed Test on Conv with stride 2 and padding 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on MaxPool\n"
     ]
    }
   ],
   "source": [
    "# Test MaxPool\n",
    "x = cnn.Add(input_tensor, input_tensor)\n",
    "y = cnn.Conv(x, conv1, 1, 1)\n",
    "z = cnn.Add(y, bias1)\n",
    "u = cnn.RELU(z)\n",
    "v = cnn.MaxPool(u, 2)\n",
    "for component in [x,y,z,u,v]:\n",
    "    component.forward()\n",
    "v.grad = np.ones_like(v.value)\n",
    "for component in [x,y,z,u, conv1, bias1]:\n",
    "    component.grad = 0\n",
    "for component in [v,u,z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "vvalue = np.array([[[17880., 18259., 18638., 19017.],\n",
    "                    [20328., 20779., 21230., 21681.]],\n",
    "                   [[27672., 28339., 29006., 29673.],\n",
    "                    [30120., 30859., 31598., 32337.]]])\n",
    "v_inputtensor_grad = np.array([[[0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.]],\n",
    "\n",
    "                               [[0., 0., 0., 0.],\n",
    "                                [1., 1., 1., 1.],\n",
    "                                [1., 1., 1., 1.],\n",
    "                                [0., 0., 0., 0.]],\n",
    "\n",
    "                               [[0., 0., 0., 0.],\n",
    "                                [1., 1., 1., 1.],\n",
    "                                [1., 1., 1., 1.],\n",
    "                                [0., 0., 0., 0.]],\n",
    "\n",
    "                               [[0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.]]])\n",
    "\n",
    "if not np.array_equal(v.value, vvalue):\n",
    "    raise FailTestError(\"The output of MaxPool is incorrect\")\n",
    "if not np.array_equal(v.input_tensor.grad, v_inputtensor_grad):\n",
    "    raise FailTestError(\"The gradient of input_tensor in MaxPool is incorrect\")\n",
    "print(\"Passed Test on MaxPool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Additional Test on MaxPool\n"
     ]
    }
   ],
   "source": [
    "# Additional Test MaxPool\n",
    "m2 = cnn.InputValue(np.arange(2*2*4).reshape((2,2,4)))\n",
    "x = cnn.Add(input_tensor, input_tensor)\n",
    "y = cnn.Conv(x, conv1, 1, 1)\n",
    "z = cnn.Add(y, bias1)\n",
    "u = cnn.RELU(z)\n",
    "v = cnn.MaxPool(u, 2)\n",
    "w = cnn.Mul(v, m2)\n",
    "for component in [x,y,z,u,v,w]:\n",
    "    component.forward()\n",
    "w.grad = np.ones_like(w.value)\n",
    "for component in [x,y,z,u,v,conv1, bias1]:\n",
    "    component.grad = 0\n",
    "for component in [w,v,u,z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "vvalue = np.array([[[17880., 18259., 18638., 19017.],\n",
    "                    [20328., 20779., 21230., 21681.]],\n",
    "                   [[27672., 28339., 29006., 29673.],\n",
    "                    [30120., 30859., 31598., 32337.]]])\n",
    "v_inputtensor_grad = np.array([[[ 0.,  0.,  0.,  0.],\n",
    "                                [ 0.,  0.,  0.,  0.],\n",
    "                                [ 0.,  0.,  0.,  0.],\n",
    "                                [ 0.,  0.,  0.,  0.]],\n",
    "\n",
    "                               [[ 0.,  0.,  0.,  0.],\n",
    "                                [ 0.,  1.,  2.,  3.],\n",
    "                                [ 4.,  5.,  6.,  7.],\n",
    "                                [ 0.,  0.,  0.,  0.]],\n",
    "\n",
    "                               [[ 0.,  0.,  0.,  0.],\n",
    "                                [ 8.,  9., 10., 11.],\n",
    "                                [12., 13., 14., 15.],\n",
    "                                [ 0.,  0.,  0.,  0.]],\n",
    "\n",
    "                               [[ 0.,  0.,  0.,  0.],\n",
    "                                [ 0.,  0.,  0.,  0.],\n",
    "                                [ 0.,  0.,  0.,  0.],\n",
    "                                [ 0.,  0.,  0.,  0.]]])\n",
    "\n",
    "if not np.array_equal(v.value, vvalue):\n",
    "    raise FailTestError(\"The output of MaxPool is incorrect\")\n",
    "if not np.array_equal(v.input_tensor.grad, v_inputtensor_grad):\n",
    "    raise FailTestError(\"The gradient of input_tensor in MaxPool is incorrect\")\n",
    "print(\"Passed Additional Test on MaxPool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on MaxPool with non-default stride\n"
     ]
    }
   ],
   "source": [
    "# For Full Credit: Test MaxPool with non-default stride\n",
    "x = cnn.Add(input_tensor, input_tensor)\n",
    "y = cnn.Conv(x, conv1, 1, 1)\n",
    "z = cnn.Add(y, bias1)\n",
    "u = cnn.RELU(z)\n",
    "v = cnn.MaxPool(u, 4, stride=4)\n",
    "for component in [x,y,z,u,v]:\n",
    "    component.forward()\n",
    "v.grad = np.ones_like(v.value)\n",
    "for component in [x,y,z,u, conv1, bias1]:\n",
    "    component.grad = 0\n",
    "for component in [v,u,z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "vvalue = np.array([[[30120., 30859., 31598., 32337.]]])\n",
    "v_inputtensor_grad = np.array([[[0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.]],\n",
    "\n",
    "                               [[0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.]],\n",
    "\n",
    "                               [[0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [1., 1., 1., 1.],\n",
    "                                [0., 0., 0., 0.]],\n",
    "\n",
    "                               [[0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.],\n",
    "                                [0., 0., 0., 0.]]])\n",
    "\n",
    "if not np.array_equal(v.value, vvalue):\n",
    "    raise FailTestError(\"The output of MaxPool is incorrect\")\n",
    "if not np.array_equal(v.input_tensor.grad, v_inputtensor_grad):\n",
    "    raise FailTestError(\"The gradient of input_tensor in MaxPool is incorrect\")\n",
    "print(\"Passed Test on MaxPool with non-default stride\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on Flatten\n"
     ]
    }
   ],
   "source": [
    "# Test Flatten\n",
    "x = cnn.Add(input_tensor, input_tensor)\n",
    "y = cnn.Conv(x, conv1, 1, 1)\n",
    "z = cnn.Add(y, bias1)\n",
    "u = cnn.RELU(z)\n",
    "v = cnn.MaxPool(u, 2)\n",
    "w = cnn.Flatten(v)\n",
    "o = cnn.Mul(w, v1)\n",
    "for component in [x,y,z,u,v,w,o]:\n",
    "    component.forward()\n",
    "o.grad = np.ones_like(o.value)\n",
    "for component in [x,y,z,u,v,w,conv1, bias1]:\n",
    "    component.grad = 0\n",
    "for component in [o,w,v,u,z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "wvalue = np.array([17880., 18259., 18638., 19017., 20328., 20779., 21230., 21681.,\n",
    "       27672., 28339., 29006., 29673., 30120., 30859., 31598., 32337.])\n",
    "w_inputtensor_grad = np.array([[[ 0.,  1.,  2.,  3.],\n",
    "        [ 4.,  5.,  6.,  7.]],\n",
    "\n",
    "       [[ 8.,  9., 10., 11.],\n",
    "        [12., 13., 14., 15.]]])\n",
    "\n",
    "if not np.array_equal(w.value, wvalue):\n",
    "    raise FailTestError(\"The output of Flatten is incorrect\")\n",
    "if not np.array_equal(w.input_tensor.grad, w_inputtensor_grad):\n",
    "    raise FailTestError(\"The gradient of input_tensor in Flatten is incorrect\")\n",
    "print(\"Passed Test on Flatten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on ReLU\n"
     ]
    }
   ],
   "source": [
    "# Test ReLU\n",
    "\n",
    "c1 = cnn.InputValue(np.sin(np.arange(3*3*4).reshape((3,3,4))).astype(np.float32))\n",
    "b1 = cnn.InputValue(np.sin(np.arange(4)).astype(np.float32))\n",
    "m1 = cnn.InputValue(np.cos(np.arange(3*3*4).reshape((3,3,4))).astype(np.float32))\n",
    "\n",
    "x = cnn.Add(c1, b1)\n",
    "y = cnn.RELU(x)\n",
    "z = cnn.Mul(y, m1)\n",
    "for component in [x,y,z]:\n",
    "    component.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "for component in [x,y]:\n",
    "    component.grad = 0\n",
    "for component in [z,y,x]:\n",
    "    component.backward()\n",
    "\n",
    "yvalue = np.array([[[0.        , 1.6829419 , 1.8185948 , 0.28224   ],\n",
    "                    [0.        , 0.        , 0.6298819 , 0.7981066 ],\n",
    "                    [0.98935825, 1.2535894 , 0.36527628, 0.        ]],\n",
    "\n",
    "                   [[0.        , 1.2616379 , 1.8999047 , 0.7914079 ],\n",
    "                    [0.        , 0.        , 0.15831017, 0.2909972 ],\n",
    "                    [0.9129453 , 1.6781266 , 0.9004461 , 0.        ]],\n",
    "\n",
    "                   [[0.        , 0.7091192 , 1.6718559 , 1.0974959 ],\n",
    "                    [0.2709058 , 0.17783707, 0.        , 0.        ],\n",
    "                    [0.5514267 , 1.8413827 , 1.4383801 , 0.        ]]], dtype=np.float32)\n",
    "yagrad = np.array([[[ 0.        ,  0.5403023 , -0.41614684, -0.9899925 ],\n",
    "                                [ 0.        ,  0.        ,  0.96017027,  0.75390226],\n",
    "                                [-0.14550003, -0.91113025, -0.8390715 ,  0.        ]],\n",
    "\n",
    "                               [[ 0.        ,  0.9074468 ,  0.13673721, -0.7596879 ],\n",
    "                                [ 0.        ,  0.        ,  0.6603167 ,  0.9887046 ],\n",
    "                                [ 0.40808207, -0.54772925, -0.99996084,  0.        ]],\n",
    "\n",
    "                               [[ 0.        ,  0.99120283,  0.6469193 , -0.29213881],\n",
    "                                [-0.9626059 , -0.74805754,  0.        ,  0.        ],\n",
    "                                [ 0.8342234 , -0.01327675, -0.8485703 ,  0.        ]]],\n",
    "                              dtype=np.float32)\n",
    "\n",
    "if not np.array_equal(np.round(y.value.astype(np.float32),3), np.round(yvalue.astype(np.float32), 3)):\n",
    "    raise FailTestError(\"The output of ReLU is incorrect\")\n",
    "if not np.array_equal(np.round(y.a.grad.astype(np.float32),3), np.round(yagrad.astype(np.float32), 3)):\n",
    "    raise FailTestError(\"The gradient of input_tensor in ReLU is incorrect\")\n",
    "print(\"Passed Test on ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Applying to the CIFAR10 dataset\n",
    "- You can refer to https://www.cs.toronto.edu/~kriz/cifar.html for details\n",
    "- Labels 0 to 9 refer to the following --- 0:airplane, 1:automobile, 2:bird, 3:cat, 4:deer, 5:dog, 6:frog, 7:horse, 8:ship, 9:truck\n",
    "- We will only use a subsample of 10000 images with 1000 of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "data = np.load('./cifar10_data/sub_data.npz')\n",
    "X = np.float32(data['imgs'])/255.\n",
    "# Reshape the valid image data to (idx, h, w, channel)\n",
    "X = X.reshape(10000, 32, 32, 3)\n",
    "y = np.float32(data['labels'])\n",
    "\n",
    "# for simplicity, let's focus on the first four classes\n",
    "# there are 4000 images in total\n",
    "sub_idx = np.where(y<=3)[0]\n",
    "X = X[sub_idx]\n",
    "y = y[sub_idx]\n",
    "\n",
    "# split in to train an test set\n",
    "train_x, test_x = X[:3000], X[3000:]\n",
    "train_y, test_y = y[:3000], y[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# below is what an image looks like\n",
    "print(train_y[0])  # 3:cat\n",
    "imgplot = plt.imshow(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peculiarity of the bias implementation\n",
    "\n",
    "Before debugging the fit function please know that this framework, in particular the ``Add`` class that has been provided, is a simplified implementation, which leads to a slightly unusual implementation of the bias for the convolutional layer.\n",
    "\n",
    "Suppose we have a convolutional layer with 6 filters that outputs an image of shape 28 x 28 x 6. In the regular implementation, the corresponding bias would be a vector of length 6.  Further given an output gradient of shape 28 x 28 x 6, the gradient of the bias would also be a vector of length 6, in which we sum the output gradients over each 28 x 28 slice.  In order to make the ``Add`` class simple and yet general, we have skipped this summation over the first two dimensions in the ``backward`` function of the ``Add`` class.  (We will likely change this in the next version.)\n",
    "\n",
    "So for convoutional layers our implementation implicitly assumes that the bias does not involve any parameter sharing, and we have a bias term for each value of the output image.  So in the above case, we work with a bias of shape 28 x 28 x 6.  (This could potentially increase the number of epochs required for convergence, but fortunately it does not in this homework.)\n",
    "\n",
    "Further, even if you do set the bias to, say 6, instead of 28 x 28 x 6, the code does not throw any error as long as your ``sgd_update_parameter`` uses\n",
    "\n",
    "`param.value = param.value - lr * param.grad`\n",
    "\n",
    "instead of \n",
    "\n",
    "`param.value -= lr * param.grad`\n",
    "\n",
    "The former works because of broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import cnn_p2 as cnn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Append <Conv> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <Conv> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <Aref> to the computational graph\n",
      "Append <Log> to the computational graph\n",
      "Append <Mul> to the computational graph\n",
      "Append <Accuracy> to the computational graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96be73823cde4fca928f4573439531a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 1.2277, accy = 0.5000, [0.673 secs]\n",
      "Congrats! You have passed the test of your fit function, your CNN model should be good to go!\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(cnn)  # important line so that the changes you made on cnn_p2.py will be reflected without restarting the kernel\n",
    "model = cnn.CNN(num_labels=4)\n",
    "\n",
    "# # Used to generate sample_params, don't uncomment the codes below\n",
    "# model.init_params_with_xavier()\n",
    "# params = model.get_param_dict()\n",
    "# with open(\"./cifar10_data/sample_params.pkl\", 'wb') as f:\n",
    "#     pickle.dump(params, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# You can use the provided sample weights for initialization to help debug\n",
    "with open(\"./cifar10_data/sample_params.pkl\", 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "model.set_params_by_dict(params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# You can use the first 2 samples to test if the gradients are correct\n",
    "X = train_x[:2]\n",
    "y = train_y[:2]\n",
    "\n",
    "# when calling fit, a computational graph will be built first, you should expect the exact lines printed\n",
    "model.fit(X, y, alpha=0.01, t=1)\n",
    "\n",
    "# # Used to generate sample_grad, don't uncomment the codes below\n",
    "# sample_grad = {}\n",
    "# for k in params.keys():\n",
    "#     sample_grad[k] = model.params[k].grad\n",
    "# with open(\"./cifar10_data/sample_grad.pkl\", 'wb') as f:\n",
    "#     pickle.dump(sample_grad, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load the sample gradient for debugging\n",
    "with open(\"./cifar10_data/sample_grad.pkl\", 'rb') as f:\n",
    "    sample_grad = pickle.load(f)\n",
    "    \n",
    "for k in params.keys():\n",
    "    if not np.array_equal(np.round(sample_grad[k], 3), np.round(model.params[k].grad, 3)):\n",
    "        raise FailTestError(f\"gradient of param {k} is incorrect\")\n",
    "print(\"Congrats! You have passed the test of your fit function, your CNN model should be good to go!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train your CNN on the whole training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Append <Conv> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <Conv> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <Aref> to the computational graph\n",
      "Append <Log> to the computational graph\n",
      "Append <Mul> to the computational graph\n",
      "Append <Accuracy> to the computational graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb86dda7bb944aca33d61282c54d66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 1.1670, accy = 0.4463, [750.427 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c540aacf0c45d89e7747b472ee2ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 1.0072, accy = 0.5680, [754.620 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd25a8a32854fb4968f386899665c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 0.8741, accy = 0.6417, [744.736 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d178ed07741b4ec6bb9c3a5aee8667e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss = 0.8541, accy = 0.6477, [734.919 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6693503cceb94ccbad686c747b904213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss = 0.7483, accy = 0.6780, [744.972 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2df81df302f4c9a85040a73c5d756d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss = 0.8185, accy = 0.6563, [752.459 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01198a07ab54d0197437f794b5e1b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss = 0.7346, accy = 0.6937, [742.679 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b128b7327e034673965b694c77b22c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss = 0.6211, accy = 0.7600, [754.562 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4162d6803e4849c1b2d83b755ef9a8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss = 0.8199, accy = 0.6957, [745.423 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b059b65db9041a98723cc65f8102dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss = 0.5273, accy = 0.7937, [750.457 secs]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(cnn)\n",
    "model = cnn.CNN(num_labels=4)\n",
    "model.init_params_with_xavier()\n",
    "# It could take as much as 1 hour to train\n",
    "model.fit(train_x, train_y, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.6180, loss = 1.1251\n"
     ]
    }
   ],
   "source": [
    "# with 10 epochs, you should be able to achieve an accuracy of over 60%, \n",
    "# which is quite good compared with 25% of random guess\n",
    "accy, loss = model.eval(test_x, test_y)\n",
    "print(\"Test accuracy = %.4f, loss = %.4f\" % (accy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
